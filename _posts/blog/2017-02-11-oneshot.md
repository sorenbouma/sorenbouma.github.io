---
layout: post
title: Simple Intro to One Shot Learning with Keras!
categories: blog
date: 2017-02-11
published: true
---

# A Simple Intro to One Shot Learning with Keras! - UNDER CONSTRUCTION


### Background:

 Conventional wisdom holds that deep neural networks are really good at classifying high dimensional data like images, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of *one-shot learning* - if you take a human who's never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with extremely high accuracy.
*(This is another one of the [things](https://dspace.mit.edu/handle/1721.1/6125 "..back in the 1960s, some of the early pioneers of AI set some undergrads the task of building a complete computer vision system over summer") humans can do that seemed trivial to us right up until we tried to make an algorithm that does it.)*

Recently however, there have been [many]("") [interesting]("") [papers]("") about one-shot learning, often with neural nets.



### The Problem Formulation - k-way one-shot learning

Before we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:

Given a test example \( x\hat{} \), a support set with 1 example each for k different categories: \( S = \{ (x_c, y_c) \}_{c=1}^{k} \), we want a model that can correctly pick which category \( x\hat{} \) belongs to.

Note that when k is higher, there are more possible classes that  \( x\hat{} \) can belong to

### The Omniglot Dataset:

The [Omniglot dataset](https://github.com/brendenlake/omniglot " Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.") is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are 20 examples, each drawn by a different person at resolution 105x105. It's sometimes referred to as the *transpose* of [MNIST](https://en.wikipedia.org/wiki/MNIST_database), since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.

For now it serves as a really good benchmark for one shot classification algorithms




### Siamese networks

One approach to getting a neural net to do one-shot classification is to give it two examples as input and train it to guess whether they have the same category.

#### Code:

Imports:


{% highlight python %}
from keras.layers import Input, Convolution2D, Lambda, merge, Dense, Flatten,MaxPooling2D
from keras.models import Model, Sequential
from keras import backend as K
from keras.optimizers import SGD
import numpy.random as rng
import numpy as np
import dill as pickle
import matplotlib.pyplot as plt
{% endhighlight %}

We need two input layers, one for each digit:
{% highlight python %}
input_shape = (105, 105, 1)
left_input = Input(input_shape)
right_input = Input(input_shape)
{% endhighlight %}

Now we define the convnet, using the same architecure as the original paper:

{% highlight python %}
convnet = Sequential()
convnet.add(Convolution2D(64,10,10,activation='relu',subsample=(1,1),input_shape=input_shape))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(128,7,7,activation='relu'))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(128,4,4,activation='relu'))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(256,4,4,activation='relu'))
convnet.add(Flatten())
convnet.add(Dense(4096,activation="sigmoid"))

{% endhighlight %}
