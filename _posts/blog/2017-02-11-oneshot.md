---
layout: post
title: Simple Intro to One Shot Learning with Keras!
categories: blog
date: 2017-02-11
published: true
---

# Simple Intro to One Shot Learning with Keras!


### Background:

 Conventional wisdom holds that deep neural networks are really good at classifying high dimensional data like images, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of *one-shot learning* - if you take a human who's never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with extremely high accuracy.
*(This is another one of the [things](https://dspace.mit.edu/handle/1721.1/6125) humans can do that seemed trivial to us right up until we tried to make an algorithm that does it.)*

Recently however, there have been

 ### The Omniglot Dataset:

The [Omniglot dataset](https://github.com/brendenlake/omniglot " Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.") is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are 20 examples, each drawn by a different person at resolution 105x105. It's sometimes referred to as the *transpose* of [MNIST](https://en.wikipedia.org/wiki/MNIST_database), since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.

For now it's a really good benchmark for one shot classification algorithms


### The Problem Formulation - k-way one-shot learning




### Siamese networks

One approach to getting a neural net to do one-shot classification is to give it

#### Code:


Since we have

{% highlight python %}

{% endhighlight %}
