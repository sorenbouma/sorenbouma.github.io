---
layout: post
title:  One Shot Learning with Siamese Networks in Keras!
categories: blog
date: 2017-02-11
published: true
image:
  feature: omniglot_header_dark.jpg

---

#  UNDER CONSTRUCTION

*[Epistemic status: I have no formal training in machine learning or statistics so some of this might be wrong/misleading, but I've tried my best.]*


### Background:

 Conventional wisdom holds that deep neural networks are really good at classifying high dimensional data like images, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of *one-shot learning* - if you take a human who's never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with extremely high accuracy.

 ![test]({{ site.url }}/images/spatula.jpg)

*Never seen a spatula before? Now's your chance to test your one shot learning ability. Email me for the correct answer*

Yet another one of the [things](https://dspace.mit.edu/handle/1721.1/6125 "..back in the 1960s, some of the early pioneers of AI set some undergrads the task of building a complete computer vision system over summer") humans can do that seemed trivial to us right up until we tried to make an algorithm do it. This ability also seems like it's obviously desirable for machine learning systems to have and an important step towards general intelligence.

Recently there have been [many]("") [interesting]("") [papers]("") about one-shot learning with neural nets and they've gotten some good results. This is a new area that really excites me, so I wanted to make a gentle intoduction to make it more accessible to fellow newcomers to deep learning.

In this post, I want to:
* Introduce and formulate the problem of one-shot learning
* Describe benchmarks for one-shot classification and give a baseline for performance
* Give an example of deep one-shot learning by reimplementing [this paper]()



## Formulating the Problem  - N-way One(or k)-Shot Learning

Before we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:

Given a test example \\( x\hat{} \\), a labelled support set with 1 example each for k different categories: \\( S =  \{ (x_i, y_i) \}_{i=1}^{N} \\), we want to learn a model that can correctly pick which category \\( x\hat{} \\) belongs to.

Note that when N is higher, there are more possible classes that  \\( x\hat{} \\) can belong to, so it's harder to  guess the correct one.

#### Ways to use deep networks for one shot learning?!

If we naively train a neural network on a one-shot as a vanilla cross-entropy-loss softmax classifier, it will *severely* overfit. Heck, even if it was a *hundred* shot learning a modern neural net would still probably overfit. Having hundreds of thousands of parameters to gradient-descend when you have only 1 example per class seems like it's obviously a bad idea.

But is optimizng lots of parameters is a bad idea for this problem, how should we use deep learning?
One answer: A neural network can learn a parametric function that's useful to a non-parametric learner. Sounds confusing? Think back to 1 nearest neighbour. This simple, non-parametric one-shot learner just classifies the test example with the same class of whatever support example is the closest in L2 distance. This works ok but it suffers from the ominous sounding [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) and so won't work well for data with thousands of dimensions like images or audio. Deep learning to the rescue! We can use a neural network to learn another similarity function to replace L2 distance and then use it in a 1 nearest-neighbour classifier. This is more or less what the siamese network I'm demonstrating does.

Note that this isn't the only way to learn a non-parametric classifier; [Lillicrap et al, Matching Networks for One-Shot Learning]() learns a complete nearest neighbours classifier end-to-end, not just a metric, comparing the test example with the entire support set, rather than just one example, and gets better results too.

This can all be viewed as a sort of meta-learning - a deep neural network learns a model that is good at one-shot learning. Because of the No-free lunch theorem we can't learn a


#### Measuring One-Shot Generalization

ML algorithms often have problems with overfitting, where they learn features that don't generalize well to examples they haven't seen before or just memorize the training set. In traditional supervised learning this is controlled by *cross validation*, where some of the training data is set aside to be the *validation set*. The model doesn't train on part of the validation set but rather is evaluated on it during training, so the user can judge how the model performs on examples it hasn't seen before.

This approach to one-shot learning(neural network approximation of non parametric classifier) is prone to overfitting too - since we train our . A one-shot learner has to be good at classifying not only *data* it hasn't seen before, but also *classes of data* it hasn't seen before. If we test a one-shot learner with examples with a class its parametric has already seen, it's not really doing one-shot learning. So to properly measure one-shot generalization, we need a validation set of examples that are of different classes to the training set.

We do have to assume that the examples in the validation task have some kind of structural similarity to examples in the training set though - it would be silly to expect to learn a one-shot imagenet classifier by training on omniglot for instance.  

### Siamese networks

[This wonderful paper](http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) is what I will be implementing in this tutorial. Kock et al's approach to getting a neural net to do one-shot classification is to give it two images and train it to guess whether they have the same category. Then when doing a one-shot classification task described above, the network can compare the test image to each image in the support set, and pick which one it thinks is most likely to be of the same category. So we want a neural net architecture that takes two images as input and outputs the probability they share the same class.

Say \\( x_1 \\) and \\( x_2 \\) are two images in our dataset, and let  \\( x_1 \sim x_2 \\) mean "\\( x_1 \\) and \\( x_2 \\) are images with the same class". Note that  \\( x_1 \sim x_2 \\) is the same as \\( x_2 \sim x_1 \\) - this means that if we reverse the order of the inputs to the neural network, the output should be the same -  \\( P(x_1  \sim x_2) \\) should equal \\( P(x_2 \sim x_1) \\).

If we we just concatenate two examples together and use them as a single input to a neural net, the n


#### History

Siamese networks, like every other cool neural net architecture, were [originally](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.590.7750&rep=rep1&type=pdf "Baldi and Chauvin(1992).") thought up in the 90s and have been applied to new problems with the power of modern GPUs.
They consist of two identical neural nets, merged together at the end like siamese twins, hence the name.  In this post I will be reimplementing the network from *Koch et al, 2015*.



### Omniglot :

The [Omniglot dataset](https://github.com/brendenlake/omniglot " Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.") is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are 20 examples, each drawn by a different person at resolution 105x105. It's sometimes referred to as the *transpose* of [MNIST](https://en.wikipedia.org/wiki/MNIST_database), since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.

It serves as a really good benchmark for one shot classification algorithms.





#### Code:

Imports:


{% highlight python %}
from keras.layers import Input, Convolution2D, Lambda, merge, Dense, Flatten,MaxPooling2D
from keras.models import Model, Sequential
from keras import backend as K
from keras.optimizers import SGD
import numpy.random as rng
import numpy as np
import dill as pickle
import matplotlib.pyplot as plt
{% endhighlight %}

Now we define the model. We need two input layers, one for each digit:
{% highlight python %}
input_shape = (105, 105, 1)
left_input = Input(input_shape)
right_input = Input(input_shape)
{% endhighlight %}

The convnet as a Sequential() model, using the same architecure as the original paper. Since the siamese network legs share weights, we only need to define it once and then call it with each leg's input layer.

{% highlight python %}
convnet = Sequential()
convnet.add(Convolution2D(64,10,10,activation='relu',subsample=(1,1),input_shape=input_shape))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(128,7,7,activation='relu'))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(128,4,4,activation='relu'))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(256,4,4,activation='relu'))
convnet.add(Flatten())
convnet.add(Dense(4096,activation="sigmoid"))
encoded_l = convnet(left_input)
encoded_r = convnet(right_input)
{% endhighlight %}

Keras allows you to merge layers together with a lambda(one line function), so we use that to get the L1 distance between the output of each leg.

{% highlight python %}
L1_distance = lambda x: K.abs(x[0]-x[1])
both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])
{% endhighlight %}

..now use the L1 distance as an input to a fully connected layer with a single output and sigmoid to squash the output into [0,1] and make it a probabilty. Compile with binary cross entropy as the loss and the same optimizer as the paper.

{% highlight python %}
prediction = Dense(1,activation='sigmoid')(both)
siamese_net = Model(input=[left_input,right_input],output=prediction)
optimizer = SGD(0.009,momentum=0.6,nesterov=True,decay=0.0003)
siamese_net.compile(loss="binary_crossentropy",optimizer=optimizer)
{% endhighlight %}
