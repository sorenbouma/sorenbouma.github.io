---
layout: post
title:  One Shot Learning with Siamese Networks in Keras!
categories: blog
date: 2017-02-11
published: true
share: true
image:
  feature: omniglot_header_dark.jpg
---

#  UNDER CONSTRUCTION - MOVE ALONG THIS PROBABLY DOESNT MAKE SENSE YET

*[Epistemic status: I have no formal training in machine learning or statistics so some of this might be wrong/misleading, but I've tried my best.]*


### Background:

 Conventional wisdom holds that deep neural networks are really good at classifying high dimensional data like images, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of *one-shot learning* - if you take a human who's never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with astoundingly high precision.


<figure>
	<a href="{{ site.url }}/images/spatula.jpg" class="image-popup"><img src="{{ site.url }}/images/spatula.jpg" alt="image"></a>
	<figcaption>Never been inside a kitchen before? Now's your chance to test your one shot learning ability! which of the images on the right is of the same type as the big image?
   Email me for the correct answer.</figcaption>
</figure>




Yet another one of the [things](https://dspace.mit.edu/handle/1721.1/6125 "..back in the 1960s, some of the early pioneers of AI set some undergrads the task of building a complete computer vision system over summer") humans can do that seemed trivial to us right up until we tried to make an algorithm do it. This ability also seems like it's obviously desirable for machine learning systems to have and an important step towards general intelligence.

Recently there have been [many]("") [interesting]("") [papers]("") about one-shot learning with neural nets and they've gotten some good results. This is a new area that really excites me, so I wanted to make a gentle intoduction to make it more accessible to fellow newcomers to deep learning.

In this post, I want to:
* Introduce and formulate the problem of one-shot learning
* Describe benchmarks for one-shot classification and give a baseline for performance
* Give an example of deep one-shot learning by reimplementing [this paper]()





## Formulating the Problem  - N-way One-Shot Learning

Before we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:

Our model is given a tiny labelled training set  \\( S \\), which has N examples, each of a different class.

\\[ S =  \{ (x_c) \}_{c=1}^{N} \\]

 It is also given \\( \hat{x} \\), the test example it has to classify. \\( \hat{x} \\) and \\(x_c \\) are all vectors \\( \in  \mathbb{R}^n \\) for some n.


Note that when N is higher, there are more possible classes that  \\( \hat{x} \\) can belong to, so it's harder to  guess the correct one. Random guessing will get \\( \frac{100\%}{n} \\) accuracy.
Here are some examples of one-shot learning tasks on the Omniglot dataset, which I describe in the next section.

<figure class="third">
	<a href="{{ site.url }}/images/task_9.png"><img src="{{ site.url }}/images/task_9.png" alt="image"></a>
	<a href="{{ site.url }}/images/task_25.png"><img src="{{ site.url }}/images/task_25.png" alt="image"></a>
	<a href="h{{ site.url }}/images/task_36.png"><img src="{{ site.url }}/images/task_36.png" alt="image"></a>
	<figcaption>9, 25 and 36 way one-shot learnng tasks.</figcaption>
</figure>



## About the data - Omniglot! :

The [Omniglot dataset](https://github.com/brendenlake/omniglot " Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.") is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are just 20 examples, each drawn by a different person at resolution 105x105. It's sometimes referred to as the *transpose* of [MNIST](https://en.wikipedia.org/wiki/MNIST_database), since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits. There is also data about the strokes used to create ech character, but we won't be using that. It's a really good benchmark for one-shot classification algorithms.

<figure class="third">
	<a href="{{ site.url }}/images/alphabets/Braille.png"><img src="{{ site.url }}/images/alphabets/Braille.png" alt="image"></a>
	<a href="{{ site.url }}/images/alphabets/Bengali.png"><img src="{{ site.url }}/images/alphabets/Bengali.png" alt="image"></a>
	<a href="{{ site.url }}/images/alphabets/Sanskrit.png"><img src="{{ site.url }}/images/alphabet/Sanskrit.png" alt="image"></a>
  <a href="{{ site.url }}/images/alphabets/Greek.png"><img src="{{ site.url }}/images/alphabet/Greek.png" alt="image"></a>
  <a href="{{ site.url }}/images/alphabets/Latin.png"><img src="{{ site.url }}/images/alphabet/Latin.png" alt="image"></a>
  <a href="{{ site.url }}/images/alphabets/Hebrew.png"><img src="{{ site.url }}/images/alphabet/Hebrew.png" alt="image"></a>

	<figcaption>A few of the alphabets from the omniglot dataset. As you can see, there's a huge variety of different symbols.</figcaption>
</figure>




#### A One-Shot Learning Baseline / 1 Nearest Neighbour

The simplest way of doing classification is with k-nearest neighbours, but since there is only one example per class we have to do 1 nearest neighbour. This is very simple, just calculate the euclidian distance of the test example from each training example and pick the closest one:

\\[ C(x\hat{}) = \underset{c}{\operatorname{argmin}} \|\| x\hat{} - x_c \|\| \\]

According to Koch et al, 1-nn gets 21.7% accuracy in 20 way one shot classification on omniglot. 21.7% doesn't sound great, but it's four times more accurate than random guessing(5%).  This is a good baseline or "sanity check" to compre future one-shot algorithms with. Humans get 95.5% accuracy, and *Hierarchical Bayesian Program Learning* from Lake et al gets 95.2% - very impressive! Comparing it with Deep Learning results is kind of "apples and oranges" though, because:
1) It used data about the strokes, not just the raw pixels
2) It involved learning a generative model for strokes, unlike deep learning it can't easily be tweaked to one-shot learn from photos of dogs and spatulas and other objects that aren't made up of brushstrokes.


#### Ways to use deep networks for one shot learning?!

If we naively train a neural network on a one-shot as a vanilla cross-entropy-loss softmax classifier, it will *severely* overfit. Heck, even if it was a *hundred* shot learning a modern neural net would still probably overfit. Having hundreds of thousands of parameters to gradient-descend when you have only a single example per class seems like it's obviously a bad idea.


It's easier for humans to one-shot learn the concept of a spatula or the letter \\( \Theta \\) becaus  they have spent a lifetime observing and learning from similar objects. It's not really fair to compare the performance of a human who's spent a lifetime having to classify objects and symbols with an untrained neural net, which imposes a very weak prior about the structure of the data. This is why most of the one-shot learning papers I've seen take the approach of *knowledge transfer* from other tasks.

Neural nets are really good at extracting useful features from structurally complex/high dimensional data, such as images. If a neural network is given training data that is similar to(but not the same as) that in the one-shot task, it might be able to learn useful features which can be used in a simple learning algorithm which doesn't require adjusting the parameters. (It still counts as one-shot learning as long as the training examples of different classes to the examples used for one-shot testing)

But how to learn these features? One way is to do *metric learning*.
If this sounds mysterious, think back to 1 nearest neighbour. This simple, non-parametric one-shot learner just classifies the test example with the same class of whatever support example is the closest in L2 distance. This works ok but L2 Distance suffers from the ominous sounding [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) and so won't work well for data with thousands of dimensions like images or video. Deep learning to the rescue! We can give a deep model many examples of images similar to the ones we want to classify and get it to learn another similarity function to replace L2 distance. This similarity measure can then be used in a 1 nearest-neighbour classifier, which is more or less what the siamese network I'm reimplementing does.






### Siamese networks


<figure>
	<a href="{{ site.url }}/images/cats.jpg" class="image-popup"><img src="{{ site.url }}/images/cats.jpg" alt="image"></a>
	<figcaption>I originally planned to have craniopagus conjoined twins as the accompanying image for this section but ultimately decided that siamese cats would go over better..</figcaption>
</figure>

[This wonderful paper](http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) is what I will be implementing in this tutorial. Koch et al's approach to getting a neural net to do one-shot classification is to give it two images and train it to guess whether they have the same category. Then when doing a one-shot classification task described above, the network can compare the test image to each image in the support set, and pick which one it thinks is most likely to be of the same category. So we want a neural net architecture that takes two images as input and outputs the probability they share the same class.

Say \\( x_1 \\) and \\( x_2 \\) are two images in our dataset, and let  \\( x_1 \circ x_2 \\) mean "\\( x_1 \\) and \\( x_2 \\) are images with the same class". Note that  \\( x_1 \circ x_2 \\) is the same as \\( x_2 \circ x_1 \\) - this means that if we reverse the order of the inputs to the neural network, the output should be the same -  \\( p(x_1  \circ x_2) \\) should equal \\( p(x_2 \circ x_1) \\). This property is called *symmetry* and siamese nets are designed around it.

If we we just concatenate two examples together and use them as a single input to a neural net, each example will be matrix multiplied(or convolved) with a different set of weights, which breaks symmetry. Sure it's possible it will eventually manage to learn the exact same weights for each input, but it would be much easier to learn a single set of weights applied to both inputs. So we could propagate both inputs through identical twin neural nets with shared parameters, then use the absolute difference as the input to a linear classifier - this is essentially what a siamese net is. Two identical twins, joined at the head, hence the name.



#### Network architecture

I'm going to describe the architecture pretty breifly because it's not the important part of the paper. Koch et al uses a *convolutional* siamese network to classify pairs of omniglot images, so the twin networks are both convolutional neural nets(CNNs). The twins each have the following architecture: convolution with 64 10x10 filters,relu -> max pool -> convolution with 128 7x7 filters, relu -> max pool -> convolution with 128 4x4 filters, relu -> max pool -> convolution with 256 4x4 filters. The twin networks reduce their inputs down to smaller and smaller 3d tensors, and their final outputs are flattened into vectors of length 4096. The absolute difference between the two vectors is used as input to a linear classifier.



<figure>
	<a href="{{ site.url }}/images/Siamese_diagram_2.png" class="image-popup"><img src="{{ site.url }}/images/Siamese_diagram_2.png" alt="image"></a>
	<figcaption>Hastily made architecture diagram.</figcaption>
</figure>



If the previous paragraph sounded like jibberish, I'd suggest checking out [cs231n](http://cs231n.github.io/convolutional-networks/) and then [colah](http://colah.github.io/posts/tags/convolutional_neural_networks.html).
Synopsis for laypeople: a convolutional layer is where you use multiple copies of the same neuron to process different areas of an image. Lots of these stacked on top of one another are really good at learning from images with gradient descent.

All up, the network has 389,51,745 parameters - this is quite a lot, even for a neural net.

The output is squashed into [0,1] with a sigmoid function. We use the label \\(t = 1\\) when the images have the same class and \\(t = 0\\) for a different class. It's trained with logistic regression. This means the loss function should be binary cross entropy between the predictions and targets.
There is also a L2 weight decay term in the loss to encourage the network to learn smaller/less noisy weights and possibly improve generalization:


\\[ L(x_1,x_2,t) = t * log(p(x_1 \circ x_2)) + (1 - t) * log(1-p(x_1 \circ x_2)) + \lambda * \|\|w\|\|_2 \\]


When it does a one-shot task, the siamese net simply classifies the test image as whatever image in the support set it thinks is most similar to the test image:
\\[ C(\hat{x},S) = \underset{c}{\operatorname{argmax}} P(\hat{x} \circ x_c) \\]



#### Observation: effective dataset size in pairwise training

One cool thing I noticed about training on pairs is that there are combinatorialy many pairs of images to train the model on, making it hard to overfit. Say we have \\( E\\) examples each of \\( E \\) classes. Since there are \\( CE \\) images total, the total number of possible pairs is given by
\\[ N_{pairs} = {\binom {CE}{2}} = {\frac {(CE) !}{2! (CE-2) !}}  \\]

For omniglot with its 20 examples of 964 training classes, this leads to 185,849,560 possible pairs, which is huge!
However, the siamese network needs examples of both same and different class pairs. There are \\( E\\) examples per class, so there will be  \\( {\binom {E}{2}}  \\) pairs for every class, which means there are  \\(N_{same} = {\binom {E}{2}}C  \\) possible pairs with the same class -  183,160 pairs for omniglot. Even though 183,160 example pairs is plenty, it's only a thousandth of the possible pairs, and the number of same-class pairs increases combinatorialy with E but only linearly with C. This is important because the siamese network should be given a 1:1 ratio of same-class and different-class pairs to train on - perhaps it implies that pairwise training is easier on datasets with lots of examples per class.

#### History

Siamese networks, like every other cool neural net architecture, were [originally](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.590.7750&rep=rep1&type=pdf "Baldi and Chauvin(1992).") thought up in the 90s and have been applied to new problems with the power of modern GPUs. Also, [Yann Lecun like them before they were cool](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf).







### Shut Up And Show Me The Code:

[Prefer to just play with a jupyter notebook? I got you fam](https://github.com/sorenbouma/keras-oneshot)

Here is the model definition. I only define the twin network's architecture once as a Sequential() model and then call it with respect to each of two input layers, this way the same parameters are used for both inputs. Then I use merge them together with absolute distance and compile the model with bce loss.

{% highlight python %}
from keras.layers import Input, Convolution2D, Lambda, merge, Dense, Flatten,MaxPooling2D
from keras.models import Model, Sequential
from keras import backend as K
from keras.optimizers import SGD
import numpy.random as rng
import numpy as np
import dill as pickle
import matplotlib.pyplot as plt



def W_init(shape,name=None):
    values = np.random.normal(loc=0,scale=1e-2,size=shape)
    return K.variable(values,name=name)


input_shape = (105, 105, 1)
left_input = Input(input_shape)
right_input = Input(input_shape)
#build convolutional network to use in each siamese 'leg'
convnet = Sequential()
convnet.add(Convolution2D(64,10,10,activation='relu',input_shape=input_shape,init=W_init))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(128,7,7,activation='relu',init=W_init))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(128,4,4,activation='relu',init=W_init))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(256,4,4,activation='relu',init=W_init))
convnet.add(Flatten())
convnet.add(Dense(4096,activation="sigmoid"))
#encode each of the two inputs into a vector with the convnet
encoded_l = convnet(left_input)
encoded_r = convnet(right_input)
#compute the l1 distance between the two encoded digits
L1_distance = lambda x: K.abs(x[0]-x[1])
both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])
prediction = Dense(1,activation='sigmoid')(both)
siamese_net = Model(input=[left_input,right_input],output=prediction)
optimizer = SGD(0.009,momentum=0.6,nesterov=True,decay=0.0003)
siamese_net.compile(loss="binary_crossentropy",optimizer=optimizer)
{% endhighlight %}


Since the omniglot dataset has exactly 20 of each sample, I arranged it as a Cx20x105x105 array to make it easier to index by class when loading data. Since there are 20 examples for all 623 classes, there are 62320 = ?? pairs to train on. In Koch et al they use a training set size of ????, but that wouldn't fit in my computers memory. I decided to just randomly sample pairs for training because I ran out of time. I'm also

Here's a class to load image pairs for the network.
{% highlight python %}
class Siamese_Loader:
    """For loading batches and testing tasks to a siamese net"""
    def __init__(self,Xtrain,Xval):
        self.Xval = Xval
        self.Xtrain = Xtrain
        self.n_classes,self.n_examples,self.w,self.h = Xtrain.shape
        self.n_val,self.n_ex_val,_,_ = Xval.shape

    def get_batch(self,n):
        """Create batch of n pairs, half same class, half different class"""
        categories = rng.choice(self.n_classes,size=(n,),replace=False)
        pairs=[np.zeros((n, self.h, self.w,1)) for i in range(2)]
        targets=np.zeros((n,))
        targets[n//2:] = 1
        for i in range(n):
            category = categories[i]
            idx_1 = rng.randint(0,self.n_examples)
            pairs[0][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,1)
            idx_2 = rng.randint(0,self.n_examples)
            #pick images of same class for 1st half, different for 2nd
            category_2 = category if i >= n//2 else (category + rng.randint(1,self.n_classes)) % self.n_classes
            pairs[1][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,1)
        return pairs, targets

    def make_oneshot_task(self,N):
        """Create pairs of test image, support set for testing N way one-shot learning. """
        categories = rng.choice(self.n_val,size=(N,),replace=False)
        indices = rng.randint(0,self.n_ex_val,size=(N,))
        true_category = categories[0]
        ex1, ex2 = rng.choice(self.n_examples,replace=False,size=(2,))
        test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,1)
        support_set = self.Xval[categories,indices,:,:]
        support_set[0,:,:] = self.Xval[true_category,ex2]
        support_set = support_set.reshape(N,self.w,self.h,1)
        pairs = [test_image,support_set]
        targets = np.zeros((N,))
        targets[0] = 1
        return pairs, targets

    def test_oneshot(self,model,N,k,verbose=0):
        """Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks"""
        pass
        n_correct = 0
        if verbose:
            print("Evaluating model on {} unique {} way one-shot learning tasks ...".format(k,N))
        for i in range(k):
            inputs, targets = self.make_oneshot_task(N)
            probs = model.predict(inputs)
            if np.argmax(probs) == 0:
                n_correct+=1
        percent_correct = (100.0*n_correct / k)
        if verbose:
            print("Got an average of {}% {} way one-shot learning accuracy".format(percent_correct,N))
        return percent_correct


{% endhighlight%}

### Results

(Put results here when the network finishes training, discuss differences from the paper)

### Discussion

We've just trained a neural network trained to do same-different pairwise classification on symbols. More importantly, we've shown that it can then get 85% accuracy in 20 way one-shot learning on symbols from other alphabets.


(Note that this isn't the only way to learn a non-parametric classifier; [Lillicrap et al, Matching Networks for One-Shot Learning]() learns a complete nearest neighbours classifier end-to-end, not just a metric, comparing the test example with the entire support set,



### Conclusion

Thanks for reading! I hope you've managed to one-shot learn the concept of one-shot learning :) If not, I'd love to answer any questions you have!
