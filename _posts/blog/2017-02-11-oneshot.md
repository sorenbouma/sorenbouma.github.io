---
layout: post
title: Simple Intro to One Shot Learning with Keras!
categories: blog
date: 2017-02-11
published: true
image:
  feature: omniglot_header_dark.jpg

---

#  UNDER CONSTRUCTION


### Background:

 Conventional wisdom holds that deep neural networks are really good at classifying high dimensional data like images, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of *one-shot learning* - if you take a human who's never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with extremely high accuracy.

 ![test]({{ site.url }}/images/spatula.jpg)



Another one of the [things](https://dspace.mit.edu/handle/1721.1/6125 "..back in the 1960s, some of the early pioneers of AI set some undergrads the task of building a complete computer vision system over summer") humans can do that seemed trivial to us right up until we tried to make an algorithm that does it.
This ability seems like it's obviously desirable for machine learning systems to have and an important step towards general intelligence.

Recently there have been [many]("") [interesting]("") [papers]("") about one-shot learning with neural nets and they've gotten some good results. This is a new area that really excites me, so I wanted to make a gentle intoduction to make it more accessible to fellow newcomers to deep learning.

In this post, I want to:
..* Introduce and formulate the problem of one-shot learning
..* Describe benchmarks for one-shot classification and give a baseline for performance
..* Give an example of deep one-shot learning by reimplementing [this paper]()



### Formulating the Problem  - k-way One-Shot Learning

Before we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:

Given a test example \\( x\hat{} \\), a labelled support set with 1 example each for k different categories: \\( S =  \{ (x_i, y_i) \}_{i=1}^{k} \\), we want to learn a model that can correctly pick which category \\( x\hat{} \\) belongs to.

Note that when k is higher, there are more possible classes that  \\( x\hat{} \\) can belong to, so it's harder to correctly guess the correct one.

ML algorithms often have problems with overfitting, where they learn features that don't generalize well to examples they haven't seen before and just memorize the training set. In traditional supervised learning this is remidied by *cross validation*, where the model doesn't train on part of the training set but rather is evaluated on it, so the user can judge how the model performs on examples it hasn't seen before. For one shot learning, we want our model to be good at classifying not only *examples* it hasn't seen before, but also *classes* it hasn't seen before. 


### Siamese networks

...[This wonderful paper](http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)

One approach to getting a neural net to do one-shot classification is to give it two images and train it to guess whether they have the same category. Then when doing a one-shot learning task described above, the network can compare the test image to each image in the support set, and pick which one it thinks is most likely to be of the same category. So we want a neural net architecture that takes two images as input and outputs the probability they share the same class.

Say \\( x_1 \\) and \\( x_2 \\) are two images in our dataset, and let  \\( x_1 \sim x_2 \\) mean "\\( x_1 \\) and \\( x_2 \\) are images with the same class". Note that  \\( x_1 \sim x_2 \\) is the same as \\( x_2 \sim x_1 \\) - this means that if we reverse the order of the inputs to the neural network, the output should be the same -  \\( P(x_1  \sim x_2) \\) should equal \\( P(x_2 \sim x_1) \\).

Siamese networks are designed to have this property(symmetry) by propagating both examples through identical layers with shared parameters


Siamese networks, like every other cool neural net architecture, were [originally](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.590.7750&rep=rep1&type=pdf "Baldi and Chauvin(1992).") thought up in the 90s and have been applied to new problems with the power of modern GPUs.
They consist of two identical neural nets, merged together at the end like siamese twins, hence the name.  In this post I will be reimplementing the network from *Koch et al, 2015*.

This network takes in two images and puts them through a deep convolutional neural network, which


### Omniglot :

The [Omniglot dataset](https://github.com/brendenlake/omniglot " Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.") is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are 20 examples, each drawn by a different person at resolution 105x105. It's sometimes referred to as the *transpose* of [MNIST](https://en.wikipedia.org/wiki/MNIST_database), since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.

It serves as a really good benchmark for one shot classification algorithms.





#### Code:

Imports:


{% highlight python %}
from keras.layers import Input, Convolution2D, Lambda, merge, Dense, Flatten,MaxPooling2D
from keras.models import Model, Sequential
from keras import backend as K
from keras.optimizers import SGD
import numpy.random as rng
import numpy as np
import dill as pickle
import matplotlib.pyplot as plt
{% endhighlight %}

Now we define the model. We need two input layers, one for each digit:
{% highlight python %}
input_shape = (105, 105, 1)
left_input = Input(input_shape)
right_input = Input(input_shape)
{% endhighlight %}

The convnet as a Sequential() model, using the same architecure as the original paper. Since the siamese network legs share weights, we only need to define it once and then call it with each leg's input layer.

{% highlight python %}
convnet = Sequential()
convnet.add(Convolution2D(64,10,10,activation='relu',subsample=(1,1),input_shape=input_shape))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(128,7,7,activation='relu'))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(128,4,4,activation='relu'))
convnet.add(MaxPooling2D())
convnet.add(Convolution2D(256,4,4,activation='relu'))
convnet.add(Flatten())
convnet.add(Dense(4096,activation="sigmoid"))
encoded_l = convnet(left_input)
encoded_r = convnet(right_input)
{% endhighlight %}

Keras allows you to merge layers together with a lambda(one line function), so we use that to get the L1 distance between the output of each leg.

{% highlight python %}
L1_distance = lambda x: K.abs(x[0]-x[1])
both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])
{% endhighlight %}

..now use the L1 distance as an input to a fully connected layer with a single output and sigmoid to squash the output into [0,1] and make it a probabilty. Compile with binary cross entropy as the loss and the same optimizer as the paper.

{% highlight python %}
prediction = Dense(1,activation='sigmoid')(both)
siamese_net = Model(input=[left_input,right_input],output=prediction)
optimizer = SGD(0.009,momentum=0.6,nesterov=True,decay=0.0003)
siamese_net.compile(loss="binary_crossentropy",optimizer=optimizer)
{% endhighlight %}
