<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator><link href="https://sorenbouma.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sorenbouma.github.io/" rel="alternate" type="text/html" /><updated>2017-03-04T13:43:35+13:00</updated><id>https://sorenbouma.github.io/</id><title type="html">Neural Tinkering</title><subtitle>Just another self serving deep learning blog</subtitle><entry><title type="html">One Shot Learning with Siamese Networks in Keras!</title><link href="https://sorenbouma.github.io/blog/oneshot/" rel="alternate" type="text/html" title="One Shot Learning with Siamese Networks in Keras!" /><published>2017-02-11T00:00:00+13:00</published><updated>2017-02-11T00:00:00+13:00</updated><id>https://sorenbouma.github.io/blog/oneshot</id><content type="html" xml:base="https://sorenbouma.github.io/blog/oneshot/">&lt;h1 id=&quot;under-construction---move-along-this-probably-doesnt-make-sense-yet&quot;&gt;UNDER CONSTRUCTION - MOVE ALONG THIS PROBABLY DOESNT MAKE SENSE YET&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;[Epistemic status: I have no formal training in machine learning or statistics so some of this might be wrong/misleading, but I’ve tried my best.]&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;background&quot;&gt;Background:&lt;/h3&gt;

&lt;p&gt;Conventional wisdom holds that deep neural networks are really good at classifying high dimensional data like images, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of &lt;em&gt;one-shot learning&lt;/em&gt; - if you take a human who’s never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with extremely high accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sorenbouma.github.io/images/spatula.jpg&quot; alt=&quot;test&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Never seen a spatula before? Now’s your chance to test your one shot learning ability! Email me for the correct answer.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Yet another one of the &lt;a href=&quot;https://dspace.mit.edu/handle/1721.1/6125&quot; title=&quot;..back in the 1960s, some of the early pioneers of AI set some undergrads the task of building a complete computer vision system over summer&quot;&gt;things&lt;/a&gt; humans can do that seemed trivial to us right up until we tried to make an algorithm do it. This ability also seems like it’s obviously desirable for machine learning systems to have and an important step towards general intelligence.&lt;/p&gt;

&lt;p&gt;Recently there have been &lt;a href=&quot;&amp;quot;&amp;quot;&quot;&gt;many&lt;/a&gt; &lt;a href=&quot;&amp;quot;&amp;quot;&quot;&gt;interesting&lt;/a&gt; &lt;a href=&quot;&amp;quot;&amp;quot;&quot;&gt;papers&lt;/a&gt; about one-shot learning with neural nets and they’ve gotten some good results. This is a new area that really excites me, so I wanted to make a gentle intoduction to make it more accessible to fellow newcomers to deep learning.&lt;/p&gt;

&lt;p&gt;In this post, I want to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Introduce and formulate the problem of one-shot learning&lt;/li&gt;
  &lt;li&gt;Describe benchmarks for one-shot classification and give a baseline for performance&lt;/li&gt;
  &lt;li&gt;Give an example of deep one-shot learning by reimplementing &lt;a href=&quot;&quot;&gt;this paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;formulating-the-problem----n-way-one-shot-learning&quot;&gt;Formulating the Problem  - N-way One-Shot Learning&lt;/h2&gt;

&lt;p&gt;Before we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:&lt;/p&gt;

&lt;p&gt;Our model is given a tiny labelled training set  \( S \), which has N examples, each of a different class.&lt;/p&gt;

&lt;p&gt;\( S =  { x_c }_{c=1}^{N} \)&lt;/p&gt;

&lt;p&gt;It is also given \( x\hat{} \), the test example it has to classify.&lt;/p&gt;

&lt;p&gt;Note that when N is higher, there are more possible classes that  \( x\hat{} \) can belong to, so it’s harder to  guess the correct one. Random guessing will get 100/N% accuracy.&lt;/p&gt;

&lt;h4 id=&quot;omniglot-&quot;&gt;Omniglot :&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/brendenlake/omniglot&quot; title=&quot; Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.&quot;&gt;Omniglot dataset&lt;/a&gt; is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are just 20 examples, each drawn by a different person at resolution 105x105. It’s sometimes referred to as the &lt;em&gt;transpose&lt;/em&gt; of &lt;a href=&quot;https://en.wikipedia.org/wiki/MNIST_database&quot;&gt;MNIST&lt;/a&gt;, since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.&lt;/p&gt;

&lt;p&gt;It serves as a really good benchmark for one shot classification algorithms.&lt;/p&gt;

&lt;h4 id=&quot;a-naive-one-shot-learner---1-nearest-neighbour&quot;&gt;A Naive One-Shot Learner - 1 Nearest Neighbour&lt;/h4&gt;

&lt;p&gt;The simplest way of doing classification is with k-nearest neighbours, but since there is only one example per class we have to do 1 nearest neighbour.&lt;/p&gt;

&lt;h4 id=&quot;ways-to-use-deep-networks-for-one-shot-learning&quot;&gt;Ways to use deep networks for one shot learning?!&lt;/h4&gt;

&lt;p&gt;If we naively train a neural network on a one-shot as a vanilla cross-entropy-loss softmax classifier, it will &lt;em&gt;severely&lt;/em&gt; overfit. Heck, even if it was a &lt;em&gt;hundred&lt;/em&gt; shot learning a modern neural net would still probably overfit. Having hundreds of thousands of parameters to gradient-descend when you have only 1 example per class seems like it’s obviously a bad idea.&lt;/p&gt;

&lt;p&gt;But is optimizng lots of parameters is a bad idea for this problem, why on earth would anyone want to use deep learning?&lt;/p&gt;

&lt;p&gt;Because they’re the best at extracting useful features from high dimensional data with complex structure like images. Maybe a neural net could learn &lt;em&gt;something&lt;/em&gt; useful from training data that is similar to that in the one-shot testing tasks. Then we could use that something&lt;/p&gt;

&lt;p&gt;Sounds confusing? Think back to 1 nearest neighbour. This simple, non-parametric one-shot learner just classifies the test example with the same class of whatever support example is the closest in L2 distance. This works ok but L2 Distance suffers from the ominous sounding &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curse of dimensionality&lt;/a&gt; and so won’t work well for data with thousands of dimensions like images or audio. Deep learning to the rescue! We can give a deep model many examples of images similar to the ones we want to classify and get it to learn another similarity function to replace L2 distance. This similarity measure can then be used in a 1 nearest-neighbour classifier, which is more or less what the siamese network I’m reimplementing does.&lt;/p&gt;

&lt;p&gt;Note that this isn’t the only way to learn a non-parametric classifier; &lt;a href=&quot;&quot;&gt;Lillicrap et al, Matching Networks for One-Shot Learning&lt;/a&gt; learns a complete nearest neighbours classifier end-to-end, not just a metric, comparing the test example with the entire support set, rather than just one example, and gets better results too.&lt;/p&gt;

&lt;p&gt;This can all be viewed as a sort of meta-learning - a deep neural network learns a model that is good at one-shot learning. Because of the no free lunch theorem we can’t learn a classifier that will work on all kinds of data, but&lt;/p&gt;

&lt;h4 id=&quot;measuring-one-shot-generalization&quot;&gt;Measuring One-Shot Generalization&lt;/h4&gt;

&lt;p&gt;ML algorithms often have problems with overfitting, where they learn features that don’t generalize well to examples they haven’t seen before or just memorize the training set. In traditional supervised learning this is controlled by &lt;em&gt;cross validation&lt;/em&gt;, where some of the training data is set aside to be the &lt;em&gt;validation set&lt;/em&gt;. The model doesn’t train on part of the validation set but rather is evaluated on it during training, so the user can judge how the model performs on examples it hasn’t seen before.&lt;/p&gt;

&lt;p&gt;This approach to one-shot learning(neural network approximation of non parametric classifier) is prone to overfitting too - since we train our . A one-shot learner has to be good at classifying not only &lt;em&gt;data&lt;/em&gt; it hasn’t seen before, but also &lt;em&gt;classes of data&lt;/em&gt; it hasn’t seen before. If we test a one-shot learner with symbols of types the metric learner has already seen, we’re no longer testing one-shot learning. So to properly measure one-shot generalization, we need a validation set of examples that are of different classes to the training set.&lt;/p&gt;

&lt;p&gt;We do have to assume that the examples in the validation task have some kind of structural similarity to examples in the training set though - it would be silly to expect to learn a one-shot imagenet classifier by training on omniglot for instance.&lt;/p&gt;

&lt;h3 id=&quot;siamese-networks&quot;&gt;Siamese networks&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf&quot;&gt;This wonderful paper&lt;/a&gt; is what I will be implementing in this tutorial. Koch et al’s approach to getting a neural net to do one-shot classification is to give it two images and train it to guess whether they have the same category. Then when doing a one-shot classification task described above, the network can compare the test image to each image in the support set, and pick which one it thinks is most likely to be of the same category. So we want a neural net architecture that takes two images as input and outputs the probability they share the same class.&lt;/p&gt;

&lt;p&gt;Say \( x_1 \) and \( x_2 \) are two images in our dataset, and let  \( x_1 \circ x_2 \) mean “\( x_1 \) and \( x_2 \) are images with the same class”. Note that  \( x_1 \circ x_2 \) is the same as \( x_2 \circ x_1 \) - this means that if we reverse the order of the inputs to the neural network, the output should be the same -  \( p(x_1  \circ x_2) \) should equal \( p(x_2 \circ x_1) \).&lt;/p&gt;

&lt;p&gt;If we we just concatenate two examples together and use them as a single input to a neural net, each example will be matrix multiplied(or convolved) with a different set of weights, which breaks symmetry. Sure it’s possible it will eventually manage to learn the exact same weights for each input, but it would be much easier to learn a single set of weights applied to both inputs. So we could propagate both inputs through identical twin neural nets with shared parameters, then use the absolute difference as the input to a linear classifier - this is essentially what a siamese net is. Two identical twins, joined at the head, hence the name.&lt;/p&gt;

&lt;h4 id=&quot;network-architecture&quot;&gt;Network architecture&lt;/h4&gt;

&lt;p&gt;I’m going to describe the architecture pretty breifly because it’s not the important part of the paper. Koch et al uses a &lt;em&gt;convolutional&lt;/em&gt; siamese network to classify pairs of omniglot images, so the twin networks are both convolutional neural nets(CNNs). The twins each have the following architecture: convolution with 64 10x10 filters,relu -&amp;gt; max pool -&amp;gt; convolution with 128 7x7 filters, relu -&amp;gt; max pool -&amp;gt; convolution with 128 4x4 filters, relu -&amp;gt; max pool -&amp;gt; convolution with 256 4x4 filters. The twin networks reduce their inputs down to smaller and smaller 3d tensors, and their final outputs are flattened into vectors of length 4096. The absolute difference between the two vectors is used as input to a linear classifier.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If the previous paragraph sounded like jibberish, I’d suggest checking out &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;cs231n&lt;/a&gt; and then &lt;a href=&quot;http://colah.github.io/posts/tags/convolutional_neural_networks.html&quot;&gt;colah&lt;/a&gt;.
Synopsis for laypeople: a convolutional layer is where you use multiple copies of the same neuron for different parts of an array. Lots of these stacked on top of one another are really good at learning from images)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The output is squashed into [0,1] with a sigmoid function. We use the label \(t = 1\) when the images have the same class and \(t = 0\) for a different class. It’s trained with logistic regression. This means the loss function should be binary cross entropy between the predictions and targets.&lt;/p&gt;

&lt;p&gt;\[ L(x_1,x_2,t) = t * log(p(x_1 \circ x_2)) + (1 - t) * log(1-p(x_1 \circ x_2))  \]&lt;/p&gt;

&lt;p&gt;There is also a L2 weight decay term in the loss to encourage the network to learn smaller/less noisy weights and possibly improve generalization:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;\[ L(x_1,x_2,t) = t * log(p(x_1 \circ x_2)) + (1 - t) * log(1-p(x_1 \circ x_2)) + \lambda *&lt;/td&gt;
      &lt;td&gt;w&lt;/td&gt;
      &lt;td&gt;_2  \]&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;history&quot;&gt;History&lt;/h4&gt;

&lt;p&gt;Siamese networks, like every other cool neural net architecture, were &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.590.7750&amp;amp;rep=rep1&amp;amp;type=pdf&quot; title=&quot;Baldi and Chauvin(1992).&quot;&gt;originally&lt;/a&gt; thought up in the 90s and have been applied to new problems with the power of modern GPUs. Also, &lt;a href=&quot;&quot;&gt;Yann Lecun like them before they were cool&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;code&quot;&gt;Code:&lt;/h4&gt;

&lt;p&gt;Imports:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Convolution2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.optimizers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;rng&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dill&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pickle&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now we define the model. We need two input layers, one for each digit:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;105&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;105&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The convnet as a Sequential() model, using the same architecure as the original paper. Since the siamese network legs share weights, we only need to define it once and then call it with each leg’s input layer.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Convolution2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subsample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Convolution2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Convolution2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Convolution2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sigmoid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoded_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoded_r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Keras allows you to merge layers together with a lambda(one line function), so we use that to get the L1 distance between the output of each leg.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;L1_distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;both&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L1_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;..now use the L1 distance as an input to a fully connected layer with a single output and sigmoid to squash the output into [0,1] and make it a probabilty. Compile with binary cross entropy as the loss and the same optimizer as the paper.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sigmoid'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;both&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;siamese_net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.009&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;momentum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nesterov&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0003&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;siamese_net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;binary_crossentropy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Since the omniglot dataset has exactly 20 of each sample, I arranged it as a Cx20x105x105 array to make it easier to index by class when loading data. Since there are 20 examples for all 623 classes, there are 62320 = ?? pairs to train on. In Koch et al they use a training set size of ????, but that wouldn’t fit in my computers memory. I decided to just randomly sample pairs for training because I ran out of time. I’m also&lt;/p&gt;</content><summary type="html">UNDER CONSTRUCTION - MOVE ALONG THIS PROBABLY DOESNT MAKE SENSE YET</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sorenbouma.github.io{&quot;feature&quot;=&gt;&quot;omniglot_header_dark.jpg&quot;}" /></entry><entry><title type="html">Hello World</title><link href="https://sorenbouma.github.io/hello-world/" rel="alternate" type="text/html" title="Hello World" /><published>2017-02-06T00:00:00+13:00</published><updated>2017-02-06T00:00:00+13:00</updated><id>https://sorenbouma.github.io/hello-world</id><content type="html" xml:base="https://sorenbouma.github.io/hello-world/">&lt;p&gt;this right here should be math:
\[ \frac{1}{n^{x}} \]&lt;/p&gt;

&lt;p&gt;more math?!&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;softmax(x) =    \frac{e^{x}}  {\sum_{i} e^{x_i}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta =  \sigma(reLU(X.W_1+b_1).W_2 + b_2)&lt;/script&gt;

&lt;p&gt;this right here should be an image:
&lt;img src=&quot;/images/anneke2.jpg&quot; alt=&quot;what the fuck&quot; /&gt;&lt;/p&gt;

&lt;p&gt;this right here should be some python code:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;to_onehot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intarray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;this right here should be a youtube video:&lt;/p&gt;

&lt;iframe width=&quot;854&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/FL5M1H-Rljk?ecver=1&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;this seems pretty easy&lt;/p&gt;</content><summary type="html">this right here should be math:
\[ \frac{1}{n^{x}} \]</summary></entry><entry><title type="html">Don’t mind me, just learning jekyll</title><link href="https://sorenbouma.github.io/blog/hello-world/" rel="alternate" type="text/html" title="Don't mind me, just learning jekyll" /><published>2017-02-06T00:00:00+13:00</published><updated>2017-02-06T00:00:00+13:00</updated><id>https://sorenbouma.github.io/blog/hello-world</id><content type="html" xml:base="https://sorenbouma.github.io/blog/hello-world/">&lt;p&gt;this right here should be math:
\[ \frac{1}{n^{x}} \]&lt;/p&gt;

&lt;p&gt;more math?!&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;softmax(x) =    \frac{e^{x}}  {\sum_{i} e^{x_i}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta =  \sigma(reLU(X.W_1+b_1).W_2 + b_2)&lt;/script&gt;

&lt;p&gt;this right here should be an image:
&lt;img src=&quot;https://sorenbouma.github.io/images/anneke2.jpg&quot; alt=&quot;what the fuck&quot; /&gt;&lt;/p&gt;

&lt;p&gt;this right here should be some python code:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;to_onehot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intarray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;this right here should be a youtube video:&lt;/p&gt;

&lt;iframe width=&quot;854&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/FL5M1H-Rljk?ecver=1&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;this seems pretty easy&lt;/p&gt;</content><summary type="html">this right here should be math:
\[ \frac{1}{n^{x}} \]</summary></entry></feed>
